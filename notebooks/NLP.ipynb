{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c358e4ed-7de5-490c-91f0-2f8935588945",
   "metadata": {},
   "source": [
    "# **Natural Language Processing Basics Notebook**\n",
    "\n",
    "This Jupyter notebook explores the fundamentals of Natural Language Processing (NLP). It starts with text preprocessing, a crucial step that involves lexical processing (tokenization, lemmatization, stemming, stop word removal, and POS tagging) to prepare the text data. Then, it dives into feature extraction techniques like Bag-of-Words, TF-IDF, and n-grams (unigrams, bigrams, skipgrams) to convert the preprocessed text into a numerical format suitable for machine learning models. Finally, the notebook explores deep learning models for NLP tasks, including Recurrent Neural Networks (RNNs) with their variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), and the powerful Transformer-based Bidirectional Encoder Representations from Transformers (BERT) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eac5de3-d589-4f1e-8403-3fb95d1b9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and dependencies load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e64057-1301-47a6-b2ba-18ca66be91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('../data/sample.txt', 'r') as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773cd59b-bc6b-449f-bf5f-4629fdae66f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog. It was a bright sunny day, and the fox was feeling particularly energetic. As it bounded through the forest, it caught sight of a dog lounging in a patch of sunlight, completely oblivious to its surroundings. The fox, being the mischievous creature that it is, decided to have a bit of fun. With a sly grin, it gathered its strength and leaped gracefully over the unsuspecting canine. The dog, startled by the sudden movement, let out a yelp of surprise before realizing what had transpired. The fox continued on its way, feeling quite pleased with itself for such a daring feat.\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d04239-4499-45a4-a7bb-9d57b9a04a1b",
   "metadata": {},
   "source": [
    "## **1. Text Preprocessing**\n",
    "\n",
    "Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves transforming raw text data into a format suitable for further analysis and modeling. This section covers various techniques used in text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1b8a9-5831-49ef-b38f-d72d804826fb",
   "metadata": {},
   "source": [
    "### **Part 1: Lexical Processing**\n",
    "Lexical processing is the foundation of working with text data in Natural Language Processing (NLP). It's essentially the first step where computers start to understand the individual building blocks of language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda8b85-cc02-41dc-abc5-ef02795d3b93",
   "metadata": {},
   "source": [
    "#### **1.1. Tokenization**\n",
    "Tokenization is the process of breaking down a sequence of text into smaller units called tokens. These tokens can be words, subwords, or other meaningful elements, depending on the tokenization method used. Common tokenization techniques include word tokenization, subword tokenization (e.g., WordPiece, BPE), and character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c6b2f9-ef32-476a-9b68-b9fa0a4d4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load \"english\" model from spacy website\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae3063a-3326-49e3-80bf-0d6172d9d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass on the corpus\n",
    "doc = nlp(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02b3707-41f6-475b-b237-3edd33d5c626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'bright',\n",
       " 'sunny']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization: Print each token (first 15)\n",
    "tokens = []\n",
    "for i in doc:\n",
    "    tokens.append(i.text)\n",
    "\n",
    "tokens[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f5cd34-4a53-4b8c-a1b1-2f4c06b31db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.',\n",
       " 'It was a bright sunny day, and the fox was feeling particularly energetic.',\n",
       " 'As it bounded through the forest, it caught sight of a dog lounging in a patch of sunlight, completely oblivious to its surroundings.',\n",
       " 'The fox, being the mischievous creature that it is, decided to have a bit of fun.',\n",
       " 'With a sly grin, it gathered its strength and leaped gracefully over the unsuspecting canine.',\n",
       " 'The dog, startled by the sudden movement, let out a yelp of surprise before realizing what had transpired.',\n",
       " 'The fox continued on its way, feeling quite pleased with itself for such a daring feat.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization: Print each sentence\n",
    "sentence = []\n",
    "for i in doc.sents:\n",
    "    sentence.append(i.text)\n",
    "\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee084e-4325-4ec0-bfae-cf52328f9053",
   "metadata": {},
   "source": [
    "#### **1.2. Lemmatization**\n",
    "Lemmatization is the process of reducing a word to its base or root form, known as the lemma. It considers the context and the part of speech of the word to determine its correct lemma. For example, the words \"went\" and \"going\" would be lemmatized to \"go.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9bae6b7-6860-4798-9877-0154a22762bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original \t Lemma\n",
      "======================\n",
      "runs \t\t run\n",
      "ran \t\t run\n",
      "fairer \t\t fair\n",
      "best \t\t good\n",
      "mice \t\t mouse\n",
      "geese \t\t geese\n",
      "better \t\t well\n",
      "went \t\t go\n",
      "were \t\t be\n",
      "has \t\t have\n",
      "am \t\t be\n",
      "easily \t\t easily\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "# List of words to experiment lemmatization on\n",
    "words = [\"runs\", \"ran\", \"fairer\", \"best\", \"mice\", \"geese\", \n",
    "         \"better\", \"went\", \"were\", \"has\", \"am\", \"easily\"]\n",
    "\n",
    "# Create a Doc object from the word list\n",
    "words_doc = nlp(\" \".join(words))\n",
    "\n",
    "# Print original words and their lemmas\n",
    "print(f\"Original \\t Lemma\")\n",
    "print(\"======================\")\n",
    "\n",
    "for i in words_doc:\n",
    "    print(f\"{i.text} \\t\\t {i.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "251de5bb-04c1-4452-b171-9318cc86895a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens \t\t Lemmas\n",
      "=======================\n",
      "The \t\t the\n",
      "quick \t\t quick\n",
      "brown \t\t brown\n",
      "fox \t\t fox\n",
      "jumps \t\t jump\n",
      "over \t\t over\n",
      "the \t\t the\n",
      "lazy \t\t lazy\n",
      "dog \t\t dog\n",
      ". \t\t .\n",
      "It \t\t it\n",
      "was \t\t be\n",
      "a \t\t a\n",
      "bright \t\t bright\n",
      "sunny \t\t sunny\n",
      "day \t\t day\n",
      ", \t\t ,\n",
      "and \t\t and\n",
      "the \t\t the\n",
      "fox \t\t fox\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Lemmatization together\n",
    "\n",
    "print(f\"Tokens \\t\\t Lemmas\")\n",
    "print(\"=======================\")\n",
    "for i in doc[:20]:\n",
    "    print(f\"{i.text} \\t\\t {i.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cd5ad-1de2-4c07-9f79-28061064044f",
   "metadata": {},
   "source": [
    "#### **1.3. Stemming**\n",
    "Stemming is a simpler approach to word normalization compared to lemmatization. It involves removing affixes (prefixes and suffixes) from words to obtain their stem or root form. However, stemming does not consider the context or part of speech, which can sometimes lead to inaccurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80339412-e600-48aa-b816-36f06e4cdf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Porter Stemming</th>\n",
       "      <th>Regexp Stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As</td>\n",
       "      <td>as</td>\n",
       "      <td>As</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bounded</td>\n",
       "      <td>bound</td>\n",
       "      <td>bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>through</td>\n",
       "      <td>through</td>\n",
       "      <td>through</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>forest</td>\n",
       "      <td>forest</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>caught</td>\n",
       "      <td>caught</td>\n",
       "      <td>caught</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sight</td>\n",
       "      <td>sight</td>\n",
       "      <td>sight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lounging</td>\n",
       "      <td>loung</td>\n",
       "      <td>loung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>patch</td>\n",
       "      <td>patch</td>\n",
       "      <td>patch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sunlight</td>\n",
       "      <td>sunlight</td>\n",
       "      <td>sunlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>completely</td>\n",
       "      <td>complet</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>oblivious</td>\n",
       "      <td>oblivi</td>\n",
       "      <td>obliv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>its</td>\n",
       "      <td>it</td>\n",
       "      <td>its</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>surroundings</td>\n",
       "      <td>surround</td>\n",
       "      <td>surrounding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tokens Porter Stemming Regexp Stemming\n",
       "0             As              as              As\n",
       "1             it              it              it\n",
       "2        bounded           bound           bound\n",
       "3        through         through         through\n",
       "4            the             the             the\n",
       "5         forest          forest          forest\n",
       "6              ,               ,               ,\n",
       "7             it              it              it\n",
       "8         caught          caught          caught\n",
       "9          sight           sight           sight\n",
       "10            of              of              of\n",
       "11             a               a               a\n",
       "12           dog             dog             dog\n",
       "13      lounging           loung           loung\n",
       "14            in              in              in\n",
       "15             a               a               a\n",
       "16         patch           patch           patch\n",
       "17            of              of              of\n",
       "18      sunlight        sunlight        sunlight\n",
       "19             ,               ,               ,\n",
       "20    completely         complet        complete\n",
       "21     oblivious          oblivi           obliv\n",
       "22            to              to              to\n",
       "23           its              it             its\n",
       "24  surroundings        surround     surrounding"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer, RegexpStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Data attribution\n",
    "tokens = word_tokenize(corpus)[25:50]    # Taking 25 tokens as data (25-50)\n",
    "porter_stems = [PorterStemmer().stem(i) for i in tokens]\n",
    "regexp_stems = [RegexpStemmer('ing$|ly$|ed$|ious$|ies$|ive$|es$|s$|ment$', min=4).stem(i) for i in tokens]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Tokens\" : tokens,\n",
    "    \"Porter Stemming\" : porter_stems,\n",
    "    \"Regexp Stemming\" : regexp_stems\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70ac45-4557-44c5-b9f3-df2a38e1ce83",
   "metadata": {},
   "source": [
    "#### **1.4. Part-of-Speech (POS) Tagging**\n",
    "This assigns grammatical labels (e.g., noun, verb, adjective) to each word in a sentence. It helps understand the function of each word within the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b6891-2406-4bc7-a082-6d8aba84d11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d9c681-e8bd-43a0-8596-1fd0b9717c8b",
   "metadata": {},
   "source": [
    "#### **1.5. Stop Word Removal**\n",
    "Stop words are commonly occurring words that often carry little or no semantic value, such as \"the,\" \"a,\" \"is,\" and \"and.\" Stop word removal is the process of filtering out these words from the text, as they can introduce noise and increase the dimensionality of the data without providing much useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14ceb1-9480-44d0-9e47-00841f98a8ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "791e615b-e45a-41c0-9b7d-ea03dd85a826",
   "metadata": {},
   "source": [
    "### **Part 2: Feature Extraction**\n",
    "\n",
    "In NLP, feature extraction is another critical step after lexical processing. It focuses on transforming the preprocessed text data into a numerical format that machine learning algorithms can understand and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d80a4d-56e1-49df-b25f-e667988e3819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ce14e87-82d3-461b-91ef-ab30aa5be1d8",
   "metadata": {},
   "source": [
    "#### **1.6. Bag of Words (BoW)**\n",
    "The Bag of Words (BoW) model is a simple and widely used technique for representing text data as vectors. In this model, each unique word in the corpus is assigned a unique index, and the text is represented as a vector of word counts or binary occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c070527-d2d1-462f-b780-0ac4edec9f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a218ba2-7a59-4ee0-a880-3ab1bc458b9c",
   "metadata": {},
   "source": [
    "#### **1.7. Term Frequency-Inverse Document Frequency (TF-IDF)**\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a numerical statistic that reflects the importance of a word in a document or corpus. It is calculated by multiplying the term frequency (TF) of a word in a document by the inverse document frequency (IDF) of that word across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547c5ae-0930-44ca-8cbc-87ef80c94d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d03865-5422-47d7-9509-46a17cd67c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad95685e-9cc3-4980-8348-907a7c155cf5",
   "metadata": {},
   "source": [
    "#### **1.8. N-grams**\n",
    "N-grams are contiguous sequences of n items (e.g., words, characters) from a given text. Common types of n-grams include:\n",
    "\n",
    "- **Unigrams**: Single words or characters.\n",
    "- **Bigrams**: Sequences of two consecutive words or characters.\n",
    "- **Skipgrams**: Sequences of words or characters with gaps in between.\n",
    "\n",
    "N-grams are often used as features in NLP tasks, capturing local context and providing more information than individual words or characters alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ac0c1-6ef1-4baa-b52a-7c7f28f442b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af54fdf6-771a-47bc-ae15-69c96f3d5029",
   "metadata": {},
   "source": [
    "## **2. Modeling**\n",
    "\n",
    "After preprocessing the text data, various machine learning and deep learning models can be employed for various NLP tasks, such as text classification, sentiment analysis, named entity recognition, and machine translation.\n",
    "\n",
    "### **Deep Learning Models for NLP**\n",
    "- **Recurrent Neural Networks (RNNs)**\n",
    "- **Long Short-Term Memory (LSTM)**\n",
    "- **Gated Recurrent Unit (GRU)**\n",
    "- **Bidirectional Encoder Representations from Transformers (BERT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed404eff-211b-4bc8-93cd-5bccbab365dd",
   "metadata": {},
   "source": [
    "#### **2.1. Recurrent Neural Networks (RNNs)**\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to process sequential data, such as text. They can capture long-term dependencies and maintain an internal state that represents the context from previous inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c55f1-e64c-4734-8b1f-ab98fef7358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a9baaa6-7e7f-402a-ac0f-a94090832733",
   "metadata": {},
   "source": [
    "#### **2.2. Long Short-Term Memory (LSTM)**\n",
    "Long Short-Term Memory (LSTM) is a variant of RNNs that addresses the vanishing gradient problem, which can occur in traditional RNNs when handling long sequences. LSTMs introduce a gating mechanism that allows them to selectively remember or forget information, making them better suited for processing and modeling long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bdb833-61a2-4fd8-8d6e-32d820ae5468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaedad24-f0be-4764-bc59-5b9e1dd22773",
   "metadata": {},
   "source": [
    "#### **2.3. Gated Recurrent Unit (GRU)**\n",
    "Gated Recurrent Unit (GRU) is another variant of RNNs that aims to solve the vanishing gradient problem. GRUs have a simpler architecture than LSTMs but often achieve comparable performance in various NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98d525-e77c-45e8-9c74-833ef6bedf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cbca4e1-cf38-4815-be21-4859d90cf367",
   "metadata": {},
   "source": [
    "#### **2.4. Bidirectional Encoder Representations from Transformers (BERT)**\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based language model that has achieved state-of-the-art results in various NLP tasks. BERT is pre-trained on a large corpus of text using a self-supervised learning approach, allowing it to capture rich contextual information and transfer that knowledge to downstream tasks with fine-tuning.\n",
    "\n",
    "This markdown provides an overview of the key steps and techniques involved in an end-to-end NLP pipeline, from text preprocessing to modeling. However, it's important to note that the specific techniques and models used may vary depending on the NLP task at hand and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997497d-967e-490b-9b37-e5d55cb6cc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a835b-55d1-47b0-b26c-2a85a214a07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf53a42-9bbe-4b9a-b8b8-e3fd2c88790b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
